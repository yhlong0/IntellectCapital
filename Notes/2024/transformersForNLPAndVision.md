# Transformers For NLP and Vision

1. An attention layer benefits from its O(1) memory time complexity. This enables the computational time complexity of O(n^2*d) to perform a dot product between each word. In machine learning, this transcribes into multiplying the representation d of each word by another word. An attention layer can thus learn all the relationships in one matrix multiplication!
2. 
