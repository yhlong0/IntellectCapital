# Artificial Intelligence

1. Hofstadter's terror was not about AI becoming too smart, too invasive, too malicious, or even too useful. Instead, he was terrified that intelligence, creativity, emotions, and maybe even consciousness itself would be too easy to produce--that what he valued most in humanity would end up being nothing more than a "bag of tricks" that a superficial set of brute force algorithms could explain the human spirit.
2. Once an AI masters human language skills, it will be in a position to expand its language skills and general knowledge by reading all human literature and absorbing the knowledge contained on millions of websites.
3. Influence of human physical body and emotions on our cognition. Perception of physical interaction with the environment shaping experience. Without the equivalent of a human body, and everything that goes along with it, a machine will never be able to learn all that's needed to pass the Turing test.
4. The core of the criticism is, that Oh Kurzweil is underestimating the complexity of reverse-engineering the human brain or the complexity of biology. But He thinks they're underestimating the power of exponential growth.
5. Long tail problems: There is a long list of very unlikely (but possible) situations. This is a problem if we rely solely on supervised learning to provide our AI system with its knowledge of the world. The situations in the tail don't show up in the training data often enough, if at all, so the system is more likely to make errors when faced with such unexpected cases.
6. Humans make mistakes all the time in driving, but human also has common sense. We have vast knowledge of the world, both its physical and its social aspects. We have a good sense of how objects are likely to behave and use this knowledge in making decisions. We can infer the reason behind salt lines on the road even if we have never driven in snow before, we know how to interact with other humans, hand signals, and body language to deal with broken traffic lights. Many people believe that until AI systems have common sense as humans do, we won't be able to trust them to be fully autonomous in complex real-world situations.
7. Animal vs no animal, the network never learns how to identify the animals, it figures out a tricky way to solve the problem, whether the image is blurred on the background or not because of the camera focus. It may be the training data is not enough.
8. ML model Biase, we have fewer images of black people, and at one time google ConvNets thinks two black people in the photo are Gorillas.
9. The success of these neural networks must be tempered with a realization that they can fail in unexpected ways because of overfitting their training data, long-tail effects, and vulnerability to hacking.
10. Transfer learning for humans is automatic, from one task to another seemingly effortless, our ability to generalize what we learn. However for Atari game-playing, go, chess, each learns its own network weights from scratch. Computers have to forget everything they learned for one game and start a new one.
11. If you shift screen up by a few pixels or change the background color, the performance plummets. This hint system has not even learned the basic concepts of wall, ceiling, paddle, ball, and tunneling.
12. The reason many parents encourage their kids to join chess clubs is because people believe that games like chess or Go teach children how to think better, logically, reason abstractly, and plan strategically. These capabilities will carry over into the rest of one's life. But AlphaGo has not learned to "think" better about anything except the game of Go. it has no ability to think about anything, to reason about anything, to make plans. The "easy things are hard" paradox of AI.
